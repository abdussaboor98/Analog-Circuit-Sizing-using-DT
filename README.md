# Analog Circuit Sizing using Decision Transformer

This project demonstrates how to use Deep Deterministic Policy Gradient (DDPG) and Decision Transformer (DT) for analog circuit sizing.

## Setup

1.  **Create and activate a conda environment:**
    ```bash
    conda env create -f environment.yml
    conda activate analoggym-env
    ```

2.  **Install PyTorch**
    Follow the instructions on the [PyTorch website](https://pytorch.org/get-started/locally/) for your specific CUDA version.

## Workflow

The typical workflow is as follows:

1.  Train a DDPG agent. This agent will interact with the circuit simulation environment and generate trajectories.
2.  Train a Decision Transformer model using the trajectories generated by the DDPG agent.
3.  Evaluate the performance of the trained DDPG agent.
4.  Evaluate the performance of the trained Decision Transformer model.

---

## 1. Train DDPG Agent

The DDPG agent is trained to optimize circuit parameters. During training, it explores the design space and learns a policy to achieve desired circuit specifications. Trajectories (state, action, reward, next_state, done, q_value) are saved during this process.

**Script:** `simple_amp_rl_rgcn.py`

This script initializes the `AMPNMCFEnv` environment, the `ActorCriticRGCN` models, and the `DDPGAgent` from `ddpg.py`.

**To run DDPG training:**

```bash
python simple_amp_rl_rgcn.py
```
You can modify training parameters like `num_steps`, `memory_size`, `batch_size`, and noise parameters directly within the `simple_amp_rl_rgcn.py` script.

**Key outputs:**

*   **Trained DDPG agent models:**
    *   Actor weights: `saved_weights/Actor_RGCN_YYYY-MM-DD_noise=TYPE_reward=X.XX.pth`
    *   Critic weights: `saved_weights/Critic_RGCN_YYYY-MM-DD_noise=TYPE_reward=X.XX.pth`
    *   The `DDPGAgent` itself also saves the best performing agent (actor and critic combined) during its `train` method, typically as `saved_agents/best_agent_episode_X_reward_Y.pth`. This is the checkpoint usually used for evaluation.
*   **Trajectories for Decision Transformer:**
    *   The `DDPGAgent`'s `train` method (called by `simple_amp_rl_rgcn.py`) saves trajectories in CSV format: `trajectories/trajectories_YYYYMMDD-HHMM.csv`. This is the primary format expected by the Decision Transformer's `TrajWindowDataset`.
*   **TensorBoard logs:** Saved in `runs/amp_ddpg_YYYYMMDD-HHMM/` (generated by the `DDPGAgent`).

**Important:**
*   The `simple_amp_rl_rgcn.py` script handles the setup and initiation of DDPG training.
*   The CSV trajectories saved by the `DDPGAgent` (e.g., `trajectories/trajectories_YYYYMMDD-HHMM.csv`) are crucial for training the Decision Transformer. Ensure the `csv_root` in your DT configuration points to this directory.

---

## 2. Train Decision Transformer (DT)

The Decision Transformer is trained on the trajectories generated by the DDPG agent. It learns to predict actions given a sequence of states, actions, and returns-to-go.

**Script:** `train.py`

**Configuration:**
Create a YAML configuration file (e.g., `configs/dt_config.yaml`) for the DT model and training. You can use `sample_config.yaml` as a template.

**Example `configs/dt_config.yaml`:**
```yaml
# Data settings
csv_root: "trajectories" # IMPORTANT: Point this to the DDPG trajectory output dir (e.g., "trajectories/")
window_K: 30
state_dim: 348  # Adjust based on your environment's state dimension
action_dim: 17   # Adjust based on your environment's action dimension
use_qvalues: false # Set to true if your DDPG trajectories have reliable q_values you want to use as returns
gamma: 0.99

# Model architecture
d_model: 128
n_heads: 8
n_layers: 6
dropout: 0.1
n_positions: 1024 # Should be >= window_K and max episode length

# Training settings
lr: 1e-4
batch_size: 64
max_iters: 200000
warmup_steps: 5000
gradient_clip_norm: 1.0
seed: 42
eval_every: 1000 # How often to evaluate on validation set
save_every: 5000 # How often to save a checkpoint
device: "cuda" # or "cpu"
```

**To run DT training:**

```bash
python train.py --config configs/dt_config.yaml --logdir training_runs/dt_run1
```

**Key arguments for `train.py`:**
*   `--config`: Path to the DT configuration YAML file.
*   `--logdir`: Directory to save DT model checkpoints and TensorBoard logs.

**Key outputs:**
*   **Trained DT model checkpoints:** Saved in `<logdir>/` (e.g., `training_runs/dt_run1/best_ckpt_stepXXXXXX.pt`).
*   **Processed trajectory data (if `log_dir` is enabled in `TrajWindowDataset`):** Saved in `<logdir>/processed_trajectories.csv`.
*   **TensorBoard logs:** Saved in `<logdir>/tensorboard/`.
*   **Copy of the config:** Saved as `<logdir>/config.yaml`.

---

## 3. Evaluate DDPG Agent

Evaluate the performance of the trained DDPG agent on the circuit sizing task.

**Script:** `evaluate_ddpg.py`

**To run DDPG evaluation:**

```bash
python evaluate_ddpg.py --ckpt <path_to_ddpg_agent.pth> --max_steps 100
```

**Key arguments for `evaluate_ddpg.py`:**
*   `--ckpt`: Path to the trained DDPG agent checkpoint (e.g., `saved_agents/best_agent_episode_X_reward_Y.pth`).
*   `--episodes`: Number of episodes to run for evaluation.

**Key outputs:**
*   **Evaluation results:** Printed to the console.
*   **Detailed results:** Saved in `evaluation_results.json`.
*   **TensorBoard logs:** Saved in `runs/evaluation_YYYYMMDD-HHMM/`.

---

## 4. Evaluate Decision Transformer (DT)

Evaluate the performance of the trained Decision Transformer model.

**Script:** `evaluate_dt.py`

**To run DT evaluation:**

```bash
python evaluate_dt.py --dt_ckpt <path_to_dt_checkpoint.pt> --dt_cfg <path_to_dt_config.yaml> --max_steps 100
```

**Key arguments for `evaluate_dt.py`:**
*   `--dt_ckpt`: Path to the trained DT model checkpoint (e.g., `training_runs/dt_run1/best_ckpt_stepXXXXXX.pt`).
*   `--dt_cfg`: Path to the DT configuration YAML file used during training (e.g., `training_runs/dt_run1/config.yaml` or your original `configs/dt_config.yaml`).
*   `--episodes`: Number of evaluation episodes.
*   `--max_steps`: Maximum number of steps per evaluation episode.

**Key outputs:**
*   **Evaluation results:** Printed to the console.
*   **TensorBoard logs:** Saved in `runs/YYYYMMDD-HHMMSS/` (a new timestamped directory is created for each evaluation run).
*   **Detailed episode information:** Saved as `episode_infos.json` within the TensorBoard log directory for that run.

---

## TensorBoard Visualization

To monitor training progress and view evaluation results, use TensorBoard:

```bash
tensorboard --logdir runs
```
Or, if you want to point to a specific run:
```bash
tensorboard --logdir training_runs/dt_run1/tensorboard # For DT training
tensorboard --logdir runs/amp_ddpg_YYYYMMDD-HHMM # For DDPG training/evaluation
```

Navigate to `http://localhost:6006` in your web browser.

---

## Acknowledgements

*   This project utilizes and adapts code from the [AnalogGym repository](https://github.com/CODA-Team/AnalogGym).
*   The Decision Transformer implementation is based on the paper: Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., & Mordatch, I. (2021). Decision Transformer: Reinforcement Learning via Sequence Modeling. [arXiv preprint arXiv:2106.01345](https://arxiv.org/abs/2106.01345).
